{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/algorithm-analysis","result":{"data":{"markdownRemark":{"id":"91c5afe1-43a6-5eef-9963-b5a8a91c1c92","html":"<p>Amortized analysis tries to bring out the average cost of an algorithm that is composed of multiple types of operations. Asymptotic analysis (Big-O) tries to compute the worse case complexity of an algorithm assuming that all operations are same (computational cost) in nature. In some case, an algorithm may composed of different types of operations whose computational speeds are way different (each other or among themselves) and treating them at same par may not accurately render the overall complexity of those algorithms. In those cases, we need to find the average cost (or complexity) of an algorithm using Amortized analysis.</p>\n<p>Let’s say we’ve an algorithm that consists of two types of operations - F (for fast) and S (for slow). Also imagine that S’s operations happen very less frequently than that of F’s operations in the algorithm. On top of that, let’s also assume that the executions of S’s operations help to let more F’s operations happen in the near future. In other words, execution of an S operation makes more F’s operations happen in the near future. In such situation, how are we going to evaluate the overall complexity of an algorithm? That’s where Amortized analysis are being used to solve such problem. </p>","fields":{"slug":"/posts/algorithm-analysis","tagSlugs":["/tag/algorithm/","/tag/array/"]},"frontmatter":{"date":"2020-06-14T23:46:37.121Z","description":"Amortized analysis - Dynamic Array","tags":["Algorithm","Array"],"title":"Amortized analysis - Dynamic Array","socialImage":"/media/heap.jpg"}}},"pageContext":{"slug":"/posts/algorithm-analysis"}}}